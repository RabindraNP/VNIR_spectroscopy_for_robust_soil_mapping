[
["index.html", "R code for Robust soil mapping at the farm scale with vis-NIR spectroscopy (Ramirez-Lopez et al., 2019) Preface", " R code for Robust soil mapping at the farm scale with vis-NIR spectroscopy (Ramirez-Lopez et al., 2019) Leo Ramirez-Lopez (BUCHI Labortechnik AG, Switzerland) Alex Wadoux (Wageningen University, Netherlands) 2019-06-18 Preface In the spirit of reproducible research, here we share the data and the computational code used for carrying out the analyses presented in our paper “Robust soil mapping at the farm scale with vis-NIR spectroscopy” which is by the way open access. Before you go thorugh this code we strongly recoment to read our paper. "],
["intro.html", "Section 1 Introduction 1.1 Paper summary 1.2 Study area 1.3 In case of comments/questions/issues 1.4 For citation or details please refer to: 1.5 Notes", " Section 1 Introduction Here you will find some basic informatzion about the paper and the R code opresented here. 1.1 Paper summary Sustainable agriculture practices are often hampered by the prohibitive costs associated with the generation of fine-resolution soil maps. Recently, several papers have been published highlighting how visible and near infrared(vis-NIR) reflectance spectroscopy may offer an alternative to address this problem by increasing the density of soil sampling and by reducing the number of conventional laboratory analyses needed. However, for farm-scale soil mapping, previous studies rarely focused on sample optimization for the calibration of vis-NIR models or on robust modelling of the spatial variation of soil properties predicted by vis-NIR spectroscopy. In the present study, we used soil vis-NIR spectroscopy models optimized in terms of both number of calibration samples and accuracy for high-resolution robust farm-scale soil mapping and addressed some of the most common pitfalls identified in previous research. We collected 910 samples from 458 locations at two depths (A, 0-0.20 m; B, 0.80-1.0 m) in the state of Sao Paulo, Brazil. All soil samples were analysed by conventional methods and scanned in the vis-NIR spectral range. With the vis-NIR spectra only, we inferred statistically the optimal set size and the best samples with which to calibrate vis-NIR models. The calibrated vis-NIR models were validated and used to predict soil properties for the rest of the samples. The prediction error of the spectroscopic model was propagated through the spatial analysis, in which robust block kriging was used to predict particle-size fractions and exchangeable calcium content for each depth. The results indicated that statistical selection of the calibration samples based on vis-NIR spectra considerably decreased the need for conventional chemical analysis for a given level of mapping accuracy. 1.2 Study area The study area is located in Brazil between the municipalities of Barra Bonita and Mineiros do Tiete (in the state of Sao Paulo). Figure 1.1: View of the study area 1.3 In case of comments/questions/issues In case you have any question comments/questions/issue related to the code presented here please go to: https://github.com/l-ramirez-lopez/VNIR_spectroscopy_for_robust_soil_mapping/issues and create a new issue. 1.4 For citation or details please refer to: Ramirez-Lopez, L., Wadoux, A. C., Franceschini, M. H. D., Terra, F. S., Marques, K. P. P., Sayao, V. M., &amp; Dematte, J. A. M. (2019). Robust soil mapping at the farm scale with vis-NIR spectroscopy. European Journal of Soil Science. 1.5 Notes Reproducibility: slight discrepancies between the results of reoported in the paper and the results you get in your R console might be expected in case you use different R package versions and different random number generators. To advanced R users: In order to make the R code more readable/interpretable we have opted for using for loops instead of vectorized functions. There are several aspects of the code that can be improved to reach better computational efficiency. "],
["required-r-packages.html", "Section 2 Required R packages", " Section 2 Required R packages In order to run all the code presented in the next sections you will need to have the following packages installed You can install the required packages using the following code requiredpackages &lt;- c(&quot;resemble&quot;, &quot;prospectr&quot;, &quot;clhs&quot;, &quot;matrixStats&quot;, &quot;doParallel&quot;, &quot;ggplot2&quot;, &quot;tidyr&quot;, &quot;tidyverse&quot;, &quot;georob&quot;, &quot;rgdal&quot;, &quot;raster&quot;, &quot;RColorBrewer&quot;) toinstall &lt;- requiredpackages[!requiredpackages %in% rownames(installed.packages())] if (length(toinstall) &gt; 0) { install.packages(toinstall) } lapply(requiredpackages, FUN = library, character.only = TRUE) "],
["required-data.html", "Section 3 Required data", " Section 3 Required data In this section you will find the instructions on how to download and read the soil dataset used in our paper. You can download the file ‘SoilNIRSaoPaulo.rds’, alternatively you can visit to the GitHub repository of the paper where the file resides. This rds file contains a data.frame with 910 rows (samples) and the following variables: Nr: An arbitrary sample number. ID: A factor indicating the sample IDs. The first character is a letter which indicates the depth layer at which the sample was collected (A: 0-20 cm and B: 80-100 cm). POINT_X: The X (geographical) coordinate. POINT_Y: The Y (geographical) coordinate. Sand: The percentage of sand contnet in the soil sample. Silt: The percentage of silt contnet in the soil sample. Clay: The percentage of clay contnet in the soil sample. Ca: The exchangeable Calcium content in the sample (\\(mmol_{c}\\) \\(kg^{−1}\\)). set: A factor indicating whether the sample is used for model’s validation or if it can be used to calibrate models. spc: A set of 307 variables representing the wavelengths from 502 nm to 2338 nm in steps of 6 nm. Further information on this dataset can be found in our paper We recommend to create a local folder (e.g. “./myworkingdirectory”). If you downloaded the file to this local folder then: workingd &lt;- &quot;/myworkingdirectory&quot; setwd(workingd) data &lt;- readRDS(&quot;SoilNIRSaoPaulo.rds&quot;) Alternatively, you can also read the file directly from the GitHub repository of the paper: nirfile &lt;- file(&quot;https://github.com/l-ramirez-lopez/VNIR_spectroscopy_for_robust_soil_mapping/raw/master/SoilNIRSaoPaulo.rds&quot;) data &lt;- readRDS(nirfile) names(data) ## [1] &quot;Nr&quot; &quot;ID&quot; &quot;POINT_X&quot; &quot;POINT_Y&quot; &quot;Sand&quot; &quot;Silt&quot; &quot;Clay&quot; ## [8] &quot;Ca&quot; &quot;set&quot; &quot;spc&quot; obg &lt;- par()$bg par(bg = rgb(0.11, 0.12, 0.17)) tcol &lt;- rgb(0.6, 0.6, 0.6, 0.8) scol &lt;- rgb(0, 1, 0, 0.5) matplot(x = as.numeric(colnames(data$spc)), y = t(data$spc), type = &quot;l&quot;, lty = 1, col = scol, xlab = &quot;wavelength (nm)&quot;, ylab = &quot;Absorbance&quot;, col.axis = tcol, col.lab = tcol) grid(lty = 1, col = tcol) Figure 3.1: Spectra in the SoilNIRSaoPaulo dataset par(bg = obg) In addition you also need a R object of class (SpatialPolygonsDataFrame of the package sp) which contains the polygon of the study area. You can also download the file (polygon.rds) containing this object by clicking here. "],
["sampling.html", "Section 4 Sampling 4.1 Optimal calibration set size identification 4.2 Plotting the results 4.3 Final selection of the calibration set", " Section 4 Sampling 4.1 Optimal calibration set size identification In this section we show how the optimal size for a calibration set is identified using the mean squared Euclidean distance (\\(msd\\)) between estimates of the probability density functions (\\(pdfs\\)) of the whole set of samples and the pdfs of subsets with different sizes. First we compute the principal components (PCs) of the NIR spectra of the whole set of calibration candidate samples. Then for each component we compute kernel density estimates (\\(KDE\\)): ## Apply standard normal variate data$spc_snv &lt;- standardNormalVariate(data$spc) ## extract the validation samples into a new set/object valida &lt;- data[data$set == &quot;validation&quot;,] ## remove the validation samples from data data &lt;- data[data$set == &quot;cal_candidate&quot;,] ## --- 2. Perform a principal component analysis ---- ## Compress the data pcaall &lt;- orthoProjection(Xr = data$spc_snv, X2 = NULL, Yr = NULL, method = &quot;pca&quot;, pcSelection = list(&quot;cumvar&quot;, 0.99), center = TRUE, scaled = FALSE) ## standardize the socres pcaall$scores.std &lt;- sweep(pcaall$scores, MARGIN = 2, STATS = pcaall$sc.sdv, FUN = &quot;/&quot;) ## compute the max and min of each score for the limits of the density estimations max.sc &lt;- colMins(pcaall$scores.std) min.sc &lt;- colMaxs(pcaall$scores.std) ## compute the mean and sd of each score for the comparisons with the samples mean.sc &lt;- colMeans(pcaall$scores.std) sd.sc &lt;- colSds(pcaall$scores.std) # number of points in the density distribution nxdens &lt;- 500 ## matrix where the density values will be stored ix &lt;- 1 sc.dens &lt;- data.frame(seq(min.sc[ix], max.sc[ix], length = nxdens), matrix(NA, nxdens, length(min.sc))) colnames(sc.dens) &lt;- c(&quot;x&quot;, paste(&quot;densc&quot;, 1:length(min.sc), sep = &quot;&quot;)) ## Kernel density estimates (KDE) of each component d.bandwidths &lt;- rep(NA, length(min.sc)) names(d.bandwidths) &lt;- colnames(pcaall$scores.std) for(i in 1:length(min.sc)){ idsty &lt;- density(pcaall$scores.std[,i], bw = &quot;nrd0&quot;, n = nxdens, from = min.sc[i], to = max.sc[i], kernel = &quot;gaussian&quot;) sc.dens[,i+1] &lt;- idsty$y d.bandwidths[i] &lt;- idsty$bw } ## Create the vector of different sample set sizes to be tested css &lt;- seq(10, 400, by = 10) Now we will iterate over the different calibration set sizes (css). We are going to use the LHS algorithm to select subsets from our set of candiate samples for calibartion. This is done using the (standardized) scores of the principal components (PCs) of the spectra (pcaall$scores.std). For each calibration subset we’ll compute the mean squared Euclidean distance (\\(msd\\)) between estimates of the probability density functions (\\(pdfs\\)) of the whole set of samples and the \\(pdfs\\) of samples in the subset. The \\(msd\\) is computed using kernel density estimates (\\(KDE\\)) of the \\(pdfs\\). To obtain reliable estimates of \\(msd\\) as a function of the sample set size, we will repeat 10 times (repetitions) the whole sampling procedure for each css, the final \\(msd\\) will be the the average of the \\(msd\\)s obtained at each iteration. The following pseudo-code summarizes the procedure to compute the \\(msd\\) (see the ‘Calibration samples’ section in our paper for more details): Input: PCs of the whole set of candidate samples (p); KDEs of the PCs of the whole set of candidate samples; Output: msd 1 for each repetition do: 2 for each proposed sampling size do: 3 select from the PCs a subset (s); 4 for each component in s and p: 5 compute the msd between KDE(s,component) and KDE(p,component); 6 end 7 end 8 end 9 aggregate the results of the msds obatined for all the repetition iterations for each sample set size First we’ll compute the \\(msd\\)s for ecah repetition and we will write each set of results into our working directory. ## Sample with LHS (latin hypercube sampling) ## These three nested loops might take a while ## (aprox. 16 min per repetition, i.e. a bit more than a couple of hours ## for the whole thing) ## (for reducing the computation time the loops ## can be vectorized using the apply family of functions. ## Furtheromre parallelization of the loop for the repetitions ## can also be applied) ## We present the computations with nested loops for interpretability ## reasons repetitions &lt;- 10 ## root name for the results of each iteration filerootname &lt;- &quot;6pcs_resultsclhs_rep&quot; ## Create the data.frame where the results will be stored ## at each iteration results.clhs &lt;- data.frame(css = css, msd = rep(NA, length(css)), mndiff = rep(NA, length(css)), sddiff = rep(NA, length(css))) for(k in 1:repetitions){ results.clhs[,-1] &lt;- NA ## Define a file name fn &lt;- paste(filerootname, k,&quot;.txt&quot;, sep = &quot;&quot;) if(fn %in% list.files()){ results.clhs &lt;- read.table(fn, header = T, sep = &quot;\\t&quot;) } iter.p &lt;- 1 + sum(rowSums(!is.na(results.clhs)) == ncol(results.clhs)) for(i in iter.p:length(css)){ set.seed(k) i.calidx &lt;- clhs(x = as.data.frame(pcaall$scores.std), size = css[i], iter = 10000) ## Compute the KDEs of each PC j.sc.dens &lt;- msd.sc &lt;- sc.dens for(j in 1:length(min.sc)){ ## use the same bandwidth (bw) as in the whole set of candidates j.sc.dens[,j + 1] &lt;- density(x = pcaall$scores.std[i.calidx,j], bw = d.bandwidths[j], n = nxdens, from = min.sc[j], to = max.sc[j], kernel = &quot;gaussian&quot;)$y } results.clhs$msd[i] &lt;- mean(colMeans((j.sc.dens[,-1] - sc.dens[,-1])^2, na.rm = T)) results.clhs$mndiff[i] &lt;- mean(abs(colMeans(pcaall$scores.std[i.calidx,]))) results.clhs$sddiff[i] &lt;- mean(abs(colSds(pcaall$scores.std[i.calidx,]) - 1)) ## write the results obtained so far... if(iter == length(nmsrepsclhs)){ final.clhs &lt;- final.clhs/iter write.table(x = final.clhs, file = &quot;6pcs_final.clhs.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE) } print(results.clhs[1:i,]) } } After executing the above nested loops, you should have obtained 10 different *.txt files in your working directorty (in this case the root name of the files is 6pcs_resultsclhs_rep[n].txt where [n] represents the repetition number). These files contain the \\(msd\\) results obtained at each repetition iteration. Now we’ll calculate the final \\(msd\\) as the average of the ones obtained at each repetition iteration. ## Specify a file name for the file where the final results will be stored finalresultsfile &lt;- &quot;6pcs_final.clhs.txt&quot; nmsrepsclhs &lt;- paste(filerootname, 1:repetitions, &quot;.txt&quot;, sep = &quot;&quot;) final.clhs &lt;- 0 for(i in nmsrepsclhs){ iter &lt;- which(i == nmsrepsclhs) results.clhs &lt;- read.table(i, header = T, sep = &quot;\\t&quot;) results.clhs$mndiff &lt;- abs(results.clhs$mndiff) final.clhs &lt;- final.clhs + results.clhs ## write a table with the final results if(iter == length(nmsrepsclhs)){ final.clhs &lt;- final.clhs/iter write.table(x = final.clhs, file = finalresultsfile, sep = &quot;\\t&quot;, row.names = FALSE) } } ## Compute the standard devitions of the msd results final.clhs_sd &lt;- 0 for(i in nmsrepsclhs){ iter &lt;- which(i == nmsrepsclhs) results.clhs &lt;- read.table(i, header = T, sep = &quot;\\t&quot;) results.clhs$mndiff &lt;- abs(results.clhs$mndiff) final.clhs_sd &lt;- (results.clhs - final.clhs_sd)^2 if(iter == length(nmsrepsclhs)){ final.clhs_sd &lt;- (final.clhs_sd/iter)^0.5 } } After executing the above code, a file with the final \\(msd\\) estimations is writen in your working directory. In addition the standard deviations of the \\(msd\\) for the different set sizes was also computed and stored in the object final.clhs_sd. 4.2 Plotting the results To plot the \\(msd\\) results using ggplot (Figure 3 in our paper): final.clhs_plot &lt;- data.frame(final.clhs[,1:2], sd_lower = final.clhs[,2] - final.clhs_sd[,2], sd_upper = final.clhs[,2] + final.clhs_sd[,2]) p.tmp &lt;- ggplot(final.clhs_plot) + geom_line(aes(x = css, msd, colour = &quot;msd&quot;)) + theme_bw() + theme(axis.title.y = element_text(face= &quot;bold.italic&quot;, colour = grey(0.2), size=18), axis.text.y = element_text(angle=0, vjust =0.5, hjust =0.5, size=14), legend.title = element_text(colour = &quot;white&quot;, size=20)) + theme(axis.title.x = element_text(face= &quot;bold&quot;, colour = grey(0.2), size=18), axis.text.x = element_text(angle = 0, vjust=0, size=14)) + theme(legend.position = &quot;none&quot;) + theme(legend.text = element_text(face=&quot;bold&quot;, colour = grey(0.2), size=18)) + labs(y = &quot;msd&quot;, x = &quot;Calibration set size&quot;) + #coord_cartesian(ylim = c(0, 8)) + theme(strip.background = element_rect(fill = &quot;grey&quot;), strip.text.x = element_text(size = 16, colour = &quot;black&quot;, angle = 0)) p.tmp + geom_ribbon(aes(ymin=sd_lower, ymax=sd_upper, x=css, colour = &quot;bands&quot;), alpha = 0.2) + scale_colour_manual(name = &#39;&#39;, values = c(&quot;bands&quot; = NA, &quot;msd&quot; = &quot;black&quot;)) 4.3 Final selection of the calibration set The previous analysis allows to infer an optimal calibration set size. This optimal size is indicated by the point at which no substantial reduction of the \\(msd\\) is observed. In our case this assestment was intuetively done by looking at the \\(msd\\) vs. \\(css\\) plot. We set the optimal calibration set size (ocss) 180 samples: # Optimal calibration sample set size ocss &lt;- 180 Once the optimal calibration set size is identified, we can propose/sample different calibration sets (solutions) containing 180 samples. In our paper we proposed 10 different solutions and we selected the one that returned the mimimum \\(msd\\) (this subset will be then used as the final calibration set in later analyses): ## Compute the maximum and minimum of each score for the limits of the ## density estimations max.sc &lt;- colMins(pcaall$scores.std) min.sc &lt;- colMaxs(pcaall$scores.std) ## Compute the mean and standard deviation of each score for the comparisons ## with the samples mean.sc &lt;- colMeans(pcaall$scores.std) sd.sc &lt;- colSds(pcaall$scores.std) ## Set a number of solutions to test solutions &lt;- 10 ## object where the sample indices of the different solutions will be stored calidx &lt;- NULL ## object where the msds will be stored results.clhs.ocss &lt;- rep(NA, length = solutions) for (i in 1:solutions) { ## set seed for the random number generation set.seed(round(i * exp(4))) ## sample the ith solution i.calidx &lt;- clhs(x = as.data.frame(pcaall$scores.std), size = ocss, iter = 10000) ## store the indices of the selected samples calidx[[i]] &lt;- i.calidx ## Compute the KDEs j.sc.dens &lt;- msd.sc &lt;- sc.dens for (j in 1:length(min.sc)) { j.sc.dens[, j + 1] &lt;- density(x = pcaall$scores.std[i.calidx, j], bw = d.bandwidths[j], n = nxdens, from = min.sc[j], to = max.sc[j], kernel = &quot;gaussian&quot;)$y } ## compute the msds results.clhs.ocss[i] &lt;- mean(colMeans((j.sc.dens[, -1] - sc.dens[, -1])^2, na.rm = T)) } ## Identify the index of the best group plot(results.clhs.ocss) ## best solution which.min(results.clhs.ocss) ## get the indices of the samples in the final solution calibration.idx &lt;- calidx[[which.min(results.clhs.ocss)]] ## get the IDs of the samples in the final solution cal_smpls &lt;- as.character(data$ID[calibration.idx]) Save the IDs of the selected calibration samples into your working directory writeLines(text = cal_smpls, con = &quot;calibration_samples_ids.txt&quot;, sep = &quot;\\n&quot;) "],
["splitting-the-data.html", "Section 5 Splitting the data", " Section 5 Splitting the data At this point we can split the data into calibration, validation, and predition sets: The calibration set comprises the samples identified in the previous section. The IDs of these samples are in the object cal_smpls. The validation set are the ones in the original set and that are labeled as validation. In the previous section the validation samples where extracted into a separate object (valida). The prediction set includes all the samples not selected for calibration and that were initially labeled as cal_candidate. To split the data you can execute the following: train &lt;- data[as.character(data$ID) %in% cal_smpls, ] pred &lt;- data[!(as.character(data$ID) %in% c(cal_smpls)), ] train$layer &lt;- as.factor(substr(train$ID, 1, 1)) valida$layer &lt;- as.factor(substr(valida$ID, 1, 1)) pred$layer &lt;- as.factor(substr(pred$ID, 1, 1)) train$ID &lt;- factor(train$ID) valida$ID &lt;- factor(valida$ID) pred$ID &lt;- factor(pred$ID) Optionally, we can get rid of all the unncessary data (R objects that will not be used from now on): ## necessary objects reqobjects &lt;- c(&quot;train&quot;, &quot;pred&quot;, &quot;valida&quot;, &quot;cal_smpls&quot;, &quot;o2rm&quot;) ## objects to be removed o2rm &lt;- ls()[!ls() %in% reqobjects] ## remove the objects rm(list = o2rm) Alternatively… ## If you have saved the IDs of the calibration samples into your working ## directory you can: cal_smpls &lt;- readLines(&quot;calibration_samples_ids.txt&quot;) and then… ## necessary objects reqobjects &lt;- c(&quot;cal_smpls&quot;, &quot;o2rm&quot;) ## objects to be removed o2rm &lt;- ls()[!ls() %in% reqobjects] ## read again the data nirfile &lt;- file(&quot;https://github.com/l-ramirez-lopez/VNIR_spectroscopy_for_robust_soil_mapping/raw/master/SoilNIRSaoPaulo.rds&quot;) data &lt;- readRDS(nirfile) ## extract the validation samples into a new set/object valida &lt;- data[data$set == &quot;validation&quot;, ] data &lt;- data[data$set == &quot;cal_candidate&quot;, ] train &lt;- data[as.character(data$ID) %in% cal_smpls, ] pred &lt;- data[!(as.character(data$ID) %in% c(cal_smpls)), ] train$layer &lt;- as.factor(substr(train$ID, 1, 1)) valida$layer &lt;- as.factor(substr(valida$ID, 1, 1)) pred$layer &lt;- as.factor(substr(pred$ID, 1, 1)) train$ID &lt;- factor(train$ID) valida$ID &lt;- factor(valida$ID) pred$ID &lt;- factor(pred$ID) "],
["transformation-of-the-particle-size-data.html", "Section 6 Transformation of the particle-size data", " Section 6 Transformation of the particle-size data Sand, silt and clay contents are reported as proportions that sum to 100%. However, models formulated for each of these fractions do not guarantee that their individual predictions sum to 100%. To avoid this compositional constraint, the particle-size data (\\(V = {clay, silt, sand}\\)) for both depths (\\(l = {A: 0–0.2 m, B: 0.8–1.0 m}\\)) were transformed using the additive log-ratio (\\(alr\\)) transformation (Odeh et al., 2003): \\[Y_{l,i} = \\frac{V_{l,i}}{V_{l,r}} \\quad \\forall \\quad i = 1,2,.. (r -1) \\quad \\forall \\quad l \\in (A,B)\\] where \\(Y_{l,i}\\) is the resulting transformed variable, \\(V_{l,i}\\) is the ith variable of the set of compositional variables (silt and clay contents) at depth \\(l\\), \\(V_{l,r}\\) designates a reference compositional variable at depth \\(l\\) and \\(r\\) is the total number of compositional variables. In ou paper, we used the sand content as reference (\\(V_{l,r}\\)). ## The above equation can be simply applied in two lines of code ## Calibration dataset ## alr for silt contnet train$alr_Silt &lt;- log(train$Silt/train$Sand) ## alr for clay contnet train$alr_Clay &lt;- log(train$Clay/train$Sand) ## Validation dataset ## alr for silt contnet valida$alr_Silt &lt;- log(valida$Silt/valida$Sand) ## alr for clay contnet valida$alr_Clay &lt;- log(valida$Clay/valida$Sand) ## Prediction dataset ## alr for silt contnet pred$alr_Silt &lt;- log(pred$Silt/pred$Sand) ## alr for clay contnet pred$alr_Clay &lt;- log(pred$Clay/pred$Sand) "],
["visnir-modelling-and-predictions.html", "Section 7 Vis–NIR modelling and predictions 7.1 Calibration and validation 7.2 Property predictions in the prediction set", " Section 7 Vis–NIR modelling and predictions A prediction model based on vis–NIR spectra was fitted for each soil property using the selected optimal calibration subset. Each model was developed using a memory-based learning (MBL) algorithm (from the resemble package). Please check the paper for additional details on the MBL lagorithm used (see section Vis–NIR modelling and predictions subsections 1-3). 7.1 Calibration and validation First define some basic aspects of the MBL which will be commmon to all the properties for which NIR modeling is going to be performed. ## Add to the train data a variable indicating to what sampling point each ## sample belongs to. This variable will be used internally by the mbl ## function during the internal validations. train$samplegroup &lt;- substr(train$ID, 2, 100) ## Dissimilarity metric: partial least squares (pls) dmetric &lt;- &quot;pls&quot; ## Threshold distances to be tested Note: the number of threshold distances ## tested here is large, it could be reduced diss2test &lt;- seq(0.3, 1.5, by = 0.05) ## Define the minimum and maximum number of neighbors that must be retained ## for each local model. Example: If with a given threshold distance only 70 ## neighbors are selected, then the function will be fored to take the ## especified minimum number of neighbors to be included in the model. In ## this case 120. We also set that the maximum number of neighbors that can ## be included can be all the samples in the calibration set. kminmax &lt;- c(120, nrow(train$spc)) ## Regression method: Weighted average pls (wapls) rmethod &lt;- &quot;wapls1&quot; ## set the maximum and minimum number of pls factors for the local wapls ## regressions pls.f &lt;- c(minpls = 5, maxpls = 20) ## Adjust some additional parameters that control some aspects of the mbl ctrl &lt;- mblControl(sm = dmetric, pcSelection = list(&quot;opc&quot;, 40), valMethod = &quot;NNv&quot;, returnDiss = TRUE, scaled = FALSE, center = TRUE) We also create some data.frames were we are going to store the prediction results for the validation set ## Create a data frame and store the validation results Squared correlation ## coefficient (R2) Root mean squared error (RMSE) Mean error (ME) (particle ## size predictions are also back-transformed to the original space to report ## the R2, RMSE and ME). valrestuls &lt;- data.frame(Property = c(&quot;Ca&quot;, &quot;alr_clay&quot;, &quot;alr_silt&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;), RMSE = NA, R2 = NA, ME = NA) ## Create a data frame and store the variance of the residuals for each layer ## (this will be later used for geostatistical analyses) residualvariances &lt;- data.frame(Property = c(&quot;Ca&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;), layerA = NA, layerB = NA, layersAB = NA) Now we perfom MBL and predictions for soil exchangeable Ca2+: ## Run the MBL and find the optimal threshold distance for neighbor selection ## and predict Ca for validation - MBL fits a wapls model to each sample in ## Xu using the calibration samples {Yr,Xr} - The internal validation ## predictions these are predictions of the nearest samples of each Xu found ## in Xr - Nearest neighbor validation (NNv) - sbl_ca &lt;- mbl(Yr = train$Ca, Xr = train$spc, Xu = valida$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = rmethod) sbl_ca ## Best threshold distance idx.best.ca &lt;- which.min(sbl_ca$nnValStats$st.rmse) best.kdiss.ca &lt;- sbl_ca$nnValStats$k.diss[idx.best.ca] ## Get the predicted values for the validation set pred_valCa &lt;- getPredictions(sbl_ca)[, idx.best.ca] ## R2 valrestuls$R2[valrestuls$Property == &quot;Ca&quot;] &lt;- cor(valida$Ca, pred_valCa)^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;Ca&quot;] &lt;- mean((valida$Ca - pred_valCa)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;Ca&quot;] &lt;- mean(valida$Ca - pred_valCa) ## Residual variances layer A residualvariances$layerA[residualvariances$Property == &quot;Ca&quot;] &lt;- var(valida$Ca[valida$layer == &quot;A&quot;] - pred_valCa[valida$layer == &quot;A&quot;]) ## layer B residualvariances$layerB[residualvariances$Property == &quot;Ca&quot;] &lt;- var(valida$Ca[valida$layer == &quot;B&quot;] - pred_valCa[valida$layer == &quot;B&quot;]) ## layers A and B residualvariances$layersAB[residualvariances$Property == &quot;Ca&quot;] &lt;- var(valida$Ca - pred_valCa) MBL and predictions for \\(alr(clay)\\)… sbl_alrclay &lt;- mbl(Yr = train$alr_Clay, Xr = train$spc, Xu = valida$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = rmethod) ## Best threshold distance idx.best.alrclay &lt;- which.min(sbl_alrclay$nnValStats$st.rmse) best.kdiss.alrclay &lt;- sbl_alrclay$nnValStats$k.diss[idx.best.alrclay] ## Get the predicted values for the validation set pred_val_alrclay &lt;- getPredictions(sbl_alrclay)[, idx.best.alrclay] ## R2 valrestuls$R2[valrestuls$Property == &quot;alr_clay&quot;] &lt;- cor(valida$alr_Clay, pred_val_alrclay)^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;alr_clay&quot;] &lt;- mean((valida$alr_Clay - pred_val_alrclay)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;alr_clay&quot;] &lt;- mean(valida$alr_Clay - pred_val_alrclay) ## Residual variances layer A residualvariances$layerA[residualvariances$Property == &quot;alr_Clay&quot;] &lt;- var(valida$alr_Clay[valida$layer == &quot;A&quot;] - pred_val_alrclay[valida$layer == &quot;A&quot;]) ## layer B residualvariances$layerB[residualvariances$Property == &quot;alr_Clay&quot;] &lt;- var(valida$alr_Clay[valida$layer == &quot;B&quot;] - pred_val_alrclay[valida$layer == &quot;B&quot;]) ## layers A and B residualvariances$layersAB[residualvariances$Property == &quot;alr_Clay&quot;] &lt;- var(valida$alr_Clay - pred_val_alrclay) …and \\(alr(silt)\\)… sbl_alrsilt &lt;- mbl(Yr = train$alr_Silt, Xr = train$spc, Xu = valida$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = rmethod) ## Best threshold distance idx.best.alrsilt &lt;- which.min(sbl_alrsilt$nnValStats$st.rmse) best.kdiss.alrsilt &lt;- sbl_alrsilt$nnValStats$k.diss[idx.best.alrsilt] ## Get the predicted values for the validation set pred_val_alrsilt &lt;- getPredictions(sbl_alrsilt)[, idx.best.alrsilt] ## R2 valrestuls$R2[valrestuls$Property == &quot;alr_silt&quot;] &lt;- cor(valida$alr_Clay, pred_val_alrsilt)^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;alr_silt&quot;] &lt;- mean((valida$alr_Clay - pred_val_alrsilt)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;alr_silt&quot;] &lt;- mean(valida$alr_Clay - pred_val_alrsilt) ## Residual variances layer A residualvariances$layerA[residualvariances$Property == &quot;alr_Silt&quot;] &lt;- var(valida$alr_Silt[valida$layer == &quot;A&quot;] - pred_val_alrsilt[valida$layer == &quot;A&quot;]) ## layer B residualvariances$layerB[residualvariances$Property == &quot;alr_Silt&quot;] &lt;- var(valida$alr_Silt[valida$layer == &quot;B&quot;] - pred_val_alrsilt[valida$layer == &quot;B&quot;]) ## layers A and B residualvariances$layersAB[residualvariances$Property == &quot;alr_Silt&quot;] &lt;- var(valida$alr_Silt - pred_val_alrsilt) Save the table of the variance of the residuals into your working directory. This data will be later used in the spatial analyses. write.table(x = residualvariances, file = &quot;vnir_residual_variances.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE) To asses the accuracies and precisions of the predictions (in the validation set) for particle-size distribution, we need to back-transform the additive log-ratio transformed variables to the original clay, silt and sand contents: ## Alr back-transformations Clay contents valClay_alr &lt;- 100 * exp(pred_val_alrclay)/(1 + exp(pred_val_alrclay) + exp(pred_val_alrsilt)) ## Silt contents valSilt_alr &lt;- 100 * exp(pred_val_alrsilt)/(1 + exp(pred_val_alrclay) + exp(pred_val_alrsilt)) ## Sand contents valSand_alr &lt;- 100 * 1/(1 + exp(pred_val_alrclay) + exp(pred_val_alrsilt)) Now we can compute the parameters to asses accuracies and precisions: ## Clay content R2 valrestuls$R2[valrestuls$Property == &quot;Clay&quot;] &lt;- (cor(valida$Clay, valClay_alr))^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;Clay&quot;] &lt;- mean((valida$Clay - valClay_alr)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;Clay&quot;] &lt;- mean(valida$Clay - valClay_alr) ## Silt content R2 valrestuls$R2[valrestuls$Property == &quot;Silt&quot;] &lt;- (cor(valida$Silt, valSilt_alr))^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;Silt&quot;] &lt;- mean((valida$Silt - valSilt_alr)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;Silt&quot;] &lt;- mean(valida$Silt - valSilt_alr) ## Sand content R2 valrestuls$R2[valrestuls$Property == &quot;Sand&quot;] &lt;- (cor(valida$Sand, valSand_alr))^2 # RMSE valrestuls$RMSE[valrestuls$Property == &quot;Sand&quot;] &lt;- mean((valida$Sand - valSand_alr)^2)^0.5 # ME valrestuls$ME[valrestuls$Property == &quot;Sand&quot;] &lt;- mean(valida$Sand - valSand_alr) Examine the valrestuls object… valrestuls 7.2 Property predictions in the prediction set After validating the vis-NIR models we can apply them to the prediction set. We can start by creating a data.frame where the predictions will be stored. In this data.frame the predicted values will be stored under the following variable names: Ca_spec, alr_Clay_spec and alr_Silt_spec: vnirpredictions &lt;- data.frame(Ca_spec = rep(NA, nrow(pred)), alr_Clay_spec = rep(NA, nrow(pred)), alr_Silt_spec = rep(NA, nrow(pred))) vnirpredictions &lt;- data.frame(Ca_spec = rep(NA, nrow(pred)), alr_Clay_spec = rep(NA, nrow(pred)), alr_Silt_spec = rep(NA, nrow(pred))) ## Add additional relevant information from the original prediction set vnirpredictions &lt;- cbind(pred[, c(&quot;ID&quot;, &quot;POINT_X&quot;, &quot;POINT_Y&quot;, &quot;set&quot;, &quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;)], vnirpredictions) ## Re-encode the set variable to &#39;prediction&#39; vnirpredictions$set &lt;- factor(&quot;prediction&quot;) For exchangeable Ca2+ predictions… # Predict Ca in the prediction set based on the optimized threshold distance # for neighbor selection sbl_ca_pred &lt;- mbl(Yr = train$Ca, Xr = train$spc, Xu = pred$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = best.kdiss.ca, k.range = kminmax, pls.c = pls.f, method = rmethod) ## get the predicted values and store them in nirpredictions vnirpredictions$Ca_spec &lt;- getPredictions(sbl_ca_pred)[, 1] For \\(alr(clay)\\) predictions… sbl_alrclay_pred &lt;- mbl(Yr = train$alr_Clay, Xr = train$spc, Xu = pred$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = best.kdiss.alrclay, k.range = kminmax, pls.c = pls.f, method = rmethod) ## get the predicted values and store them in nirpredictions vnirpredictions$alr_Clay_spec &lt;- getPredictions(sbl_alrclay_pred)[, 1] For \\(alr(silt)\\) predictions… sbl_alrsilt_pred &lt;- mbl(Yr = train$alr_Silt, Xr = train$spc, Xu = pred$spc, mblCtrl = ctrl, group = train$samplegroup, dissUsage = &quot;none&quot;, k.diss = best.kdiss.alrsilt, k.range = kminmax, pls.c = pls.f, method = rmethod) ## get the predicted values and store them in nirpredictions vnirpredictions$alr_Silt_spec &lt;- getPredictions(sbl_alrsilt_pred)[, 1] Examine the vnirpredictions object… vnirpredictions summary(vnirpredictions) "],
["prepare-the-vis-nir-augmented-dataset.html", "Section 8 Prepare the vis-NIR augmented dataset", " Section 8 Prepare the vis-NIR augmented dataset Here we’ll create the data that will be used for Spatial modelling. This dataset will contain Nr: The arbitrary sample number. ID: The factor indicating the sample IDs. POINT_X: The X (geographical) coordinate. POINT_Y: The Y (geographical) coordinate. layer: A factor indicating the depth layer at which the sample was collected (A: 0-20 cm and B: 80-100 cm). set: A factor indicating whether the sample was used for vis-NIR calibrations (train), for vis-NIR predictions (prediction) or if it belongs to model’s validation (validation). The samples labeled as validation are the same samples initially labeled as validation in the original dataset. Ca: The exchangeable Calcium content in the sample (\\(mmol_{c}\\) \\(kg^{−1}\\), measured by conventional laboratory methods) Clay: The percentage of clay contnet in the soil sample (measured by conventional laboratory methods). Silt: The percentage of silt contnet in the soil sample (measured by conventional laboratory methods). Sand: The percentage of sand contnet in the soil sample (measured by conventional laboratory methods). alr_Clay: The additive log-ratio transformed clay contnets (measured by conventional laboratory methods). alr_Silt: The additive log-ratio transformed silt contnets (measured by conventional laboratory methods). Ca_spec: This is the vis-NIR augmented exchangeable Ca2+ contents. alr_Clay_spec: This is the vis-NIR augmented additive log-ratio transformed clay contnets. alr_Silt_spec: This is the vis-NIR augmented additive log-ratio transformed silt contnets. For the vis-NIR augmented variables (alr_Clay_spc, alr_Silt_spc and Ca_spec) there are three classes of values: The values of the samples that are labeled as train come from the conventional laboratory methods (e.g. for Ca_spec the values of these samples for this variable are identical to the corresponding values in the variable Ca). The values of the samples that are labeled as prediction come from the predictions done with the respective vis-NIR model. The values of the samples that are labeled as validation are treated as missing (i.e. NAs). ## samples for the set &#39;prediction&#39; vnirpredictions ## samples for the set &#39;train&#39; vnirtrain &lt;- train[, c(&quot;ID&quot;, &quot;POINT_X&quot;, &quot;POINT_Y&quot;, &quot;set&quot;, &quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;)] vnirtrain$set &lt;- factor(&quot;train&quot;) vnirtrain$Ca_spec &lt;- vnirtrain$Ca vnirtrain$alr_Clay_spec &lt;- vnirtrain$alr_Clay vnirtrain$alr_Silt_spec &lt;- vnirtrain$alr_Silt ## samples for the set &#39;validation&#39; vnirvalidation &lt;- valida[, c(&quot;ID&quot;, &quot;POINT_X&quot;, &quot;POINT_Y&quot;, &quot;set&quot;, &quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;)] vnirvalidation$set &lt;- factor(vnirvalidation$set) vnirvalidation$Ca_spec &lt;- NA vnirvalidation$alr_Clay_spec &lt;- NA vnirvalidation$alr_Silt_spec &lt;- NA Now create a single data.frame containing the three data sets… vniraugmented &lt;- rbind(vnirtrain, vnirpredictions, vnirvalidation) vniraugmented$layer &lt;- factor(substr(vniraugmented$ID, 1, 1)) ## Reorganize the variables vniraugmented &lt;- vniraugmented[, c(&quot;ID&quot;, &quot;POINT_X&quot;, &quot;POINT_Y&quot;, &quot;layer&quot;, &quot;set&quot;, &quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;, &quot;Ca_spec&quot;, &quot;alr_Clay_spec&quot;, &quot;alr_Silt_spec&quot;)] Compute some statistics for the final data set… ## Names of the properties props &lt;- c(&quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;, &quot;Ca_spec&quot;, &quot;alr_Clay_spec&quot;, &quot;alr_Silt_spec&quot;) ## Compute the statistics: mean, standard deviation and the quantiles (&#39;0%&#39;, ## &#39;25%&#39;, &#39;50%&#39;, &#39;75%&#39; and&#39;100%&#39;) statsprops &lt;- aggregate(vniraugmented[, props], by = list(set = vniraugmented$set, layer = vniraugmented$layer), FUN = function(x) { c(mean = mean(as.matrix(x), na.rm = TRUE), sd = sd(as.matrix(x), na.rm = TRUE), quantile(x, na.rm = TRUE)) }) ## Reorganize the object containing the results of the statistics statsprops &lt;- lapply(props, FUN = function(x, object, ids) { object &lt;- cbind(object[, keep], as.data.frame(statsquant[[x]])) }, object = statsprops, ids = c(&quot;set&quot;, &quot;layer&quot;)) names(statsprops) &lt;- props statsprops &lt;- do.call(&quot;rbind&quot;, statsprops) statsprops$property &lt;- gsub(&quot;.[0-9]&quot;, &quot;&quot;, rownames(statsprops)) statsprops[is.na(statsprops)] &lt;- NA ## Reorganize the order of the variables statsprops &lt;- statsprops[, c(&quot;set&quot;, &quot;layer&quot;, &quot;property&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;0%&quot;, &quot;25%&quot;, &quot;50%&quot;, &quot;75%&quot;, &quot;100%&quot;)] statsprops Optionally, save this data in your working directory write.table(x = vniraugmented, file = &quot;vniraugmented.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE) "],
["spatial-modeling.html", "Section 9 Spatial modeling 9.1 Robust fitting of the spatial models 9.2 Accounting for vis-NIR model errors in the spatial models 9.3 Validation of the spatial models 9.4 Mapping", " Section 9 Spatial modeling In case you have previosly saved both, the vis-NIR augmented data set and the table of the variance of the residuals into your working directory, and you do not have these data in your R enviroment you can execute the code below: ## vniraugmented.txt is spuppposed to be saved in your working directory vniraugmented &lt;- read.table(file = &quot;vniraugmented.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) If you have saved the table of the variance of the residuals into your working directory you can execute the code below: ## vnir_residual_variances.txt is spuppposed to be saved in your working ## directory residualvariances &lt;- read.table(file = &quot;vnir_residual_variances.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) Alternatively, you can clean your R enviroment and leave only the data that will be used for the spatial analyses: ## necessary objects reqobjects2 &lt;- c(&quot;vniraugmented&quot;, &quot;residualvariances&quot;) ## objects to be removed o2rm2 &lt;- ls()[!ls() %in% reqobjects2] ## remove the objects rm(list = o2rm2) Split the data and organize it by layers and spatial fit and validation sets vniraugmented &lt;- as_tibble(vniraugmented) ## split the data sets by layer and by spatial fit and spatial validation fitlayera &lt;- vniraugmented %&gt;% filter(layer == &quot;A&quot;, set != &quot;validation&quot;) fitlayerb &lt;- vniraugmented %&gt;% filter(layer == &quot;B&quot;, set != &quot;validation&quot;) vallayera &lt;- vniraugmented %&gt;% filter(layer == &quot;A&quot;, set == &quot;validation&quot;) vallayerb &lt;- vniraugmented %&gt;% filter(layer == &quot;B&quot;, set == &quot;validation&quot;) plot(vallayera$POINT_X, vallayera$POINT_Y, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) points(fitlayera$POINT_X, fitlayera$POINT_Y, col = &quot;red&quot;, cex = 1.5) 9.1 Robust fitting of the spatial models Specify some basic aspects/parameters required for fitting the spatial models… ## Define a lag distance for the estimation of the variagram lagdist &lt;- seq(0, 1500, by = 100) ## Choose the variogram model varmodel &lt;- &quot;RMexp&quot; ## Define what parameters need to be adjusted to fit the geo model fitparam &lt;- default.fit.param(scale = FALSE, alpha = TRUE, variance = FALSE) ## A tuning constant for the robust REML algorithm for the spatial models ## (see tuniing.psi parameter of the georob function) tpsi &lt;- 2000 ## Control some aspects of the spatial predictions gcntrl &lt;- control.predict.georob(extended.output = TRUE, full.covmat = TRUE) Define the tables where the fitted variogram parameters will be stored ## The variables for which a spatial model will be fitted gprops &lt;- c(&quot;Ca&quot;, &quot;alr_Clay&quot;, &quot;alr_Silt&quot;, &quot;Ca_spec&quot;, &quot;alr_Clay_spec&quot;, &quot;alr_Silt_spec&quot;) ## names of the variogram parameters varparamames &lt;- c(&quot;variance&quot;, &quot;snugget&quot;, &quot;nugget&quot;, &quot;scale&quot;) ## Create the table variogtablea &lt;- data.frame(properties = gprops, data = rep(c(&quot;Laboratory&quot;, &quot;vis-NIR augmented&quot;), each = length(gprops)/2), variance = NA, snugget = NA, nugget = NA, scale = NA) variogtableb &lt;- variogtablea 9.1.1 Laboratory-based data Here we fit the spatial models of the soil properties whose values comes from conventional laboratory analyes only… 9.1.1.1 Layer A Exchangeable Ca2+… ## Check the sample variogram vario_Ca_lab_a &lt;- sample.variogram(object = fitlayera$Ca, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_Ca_lab_a$lag.dist, y = vario_Ca_lab_a$gamma, ylim = c(0, max(vario_Ca_lab_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;Ca, laboratory - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_Ca_lab_a &lt;- c(variance = 135, nugget = 10, scale = 920) ## Fit Ca_lab_a &lt;- georob(Ca ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_Ca_lab_a, fit.param = fitparam, tuning.psi = tpsi) summary(Ca_lab_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;Ca&quot;, varparamames] &lt;- Ca_lab_a$variogram.object[[1]]$param[varparamames] \\(alr(clay)\\) ## Check the sample variogram vario_alr_Clay_lab_a &lt;- sample.variogram(object = fitlayera$alr_Clay, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Clay_lab_a$lag.dist, y = vario_alr_Clay_lab_a$gamma, ylim = c(0, max(vario_alr_Clay_lab_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(clay) laboratory - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Clay_lab_a &lt;- c(variance = 0.8, nugget = 0.3, scale = 1220) ## Fit alr_Clay_lab_a &lt;- georob(alr_Clay ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Clay_lab_a, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Clay_lab_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;alr_Clay&quot;, varparamames] &lt;- alr_Clay_lab_a$variogram.object[[1]]$param[varparamames] \\(alr(silt)\\) ## Check the sample variogram vario_alr_Silt_lab_a &lt;- sample.variogram(object = fitlayera$alr_Silt, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Silt_lab_a$lag.dist, y = vario_alr_Silt_lab_a$gamma, ylim = c(0, max(vario_alr_Silt_lab_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(silt) laboratory - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Silt_lab_a &lt;- c(variance = 1.31, nugget = 0.3, scale = 812) ## Fit alr_Silt_lab_a &lt;- georob(alr_Silt ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Silt_lab_a, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Silt_lab_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;alr_Silt&quot;, varparamames] &lt;- alr_Silt_lab_a$variogram.object[[1]]$param[varparamames] 9.1.1.2 Layer B Exchangeable Ca2+… ## Check the sample variogram vario_Ca_lab_b &lt;- sample.variogram(object = fitlayerb$Ca, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_Ca_lab_b$lag.dist, y = vario_Ca_lab_b$gamma, ylim = c(0, max(vario_Ca_lab_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;Ca, laboratory - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_Ca_lab_b &lt;- c(variance = 127, nugget = 0.1, scale = 850) ## Fit Ca_lab_b &lt;- georob(Ca ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_Ca_lab_b, fit.param = fitparam, tuning.psi = tpsi) summary(Ca_lab_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;Ca&quot;, varparamames] &lt;- Ca_lab_b$variogram.object[[1]]$param[varparamames] \\(alr(clay)\\) ## Check the sample variogram vario_alr_Clay_lab_b &lt;- sample.variogram(object = fitlayerb$alr_Clay, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Clay_lab_b$lag.dist, y = vario_alr_Clay_lab_b$gamma, ylim = c(0, max(vario_alr_Clay_lab_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(clay) laboratory - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Clay_lab_b &lt;- c(variance = 1.17, nugget = 0.1, scale = 973, alpha = 0.69) ## Fit alr_Clay_lab_b &lt;- georob(alr_Clay ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Clay_lab_b, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Clay_lab_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;alr_Clay&quot;, varparamames] &lt;- alr_Clay_lab_b$variogram.object[[1]]$param[varparamames] \\(alr(silt)\\) ## Check the sample variogram vario_alr_Silt_lab_b &lt;- sample.variogram(object = fitlayerb$alr_Silt, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Silt_lab_b$lag.dist, y = vario_alr_Silt_lab_b$gamma, ylim = c(0, max(vario_alr_Silt_lab_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(silt) laboratory - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Silt_lab_b &lt;- c(variance = 1.1, nugget = 0.2, scale = 423) ## Fit alr_Silt_lab_b &lt;- georob(alr_Silt ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Silt_lab_b, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Silt_lab_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;alr_Silt&quot;, varparamames] &lt;- alr_Silt_lab_b$variogram.object[[1]]$param[varparamames] 9.1.2 Augmented vis-NIR data Here we fit the spatial models of the soil properties whose values come from the vis-NIR augmented data… 9.1.2.1 Layer A Exchangeable Ca2+ (vis-NIR augmented) … # Check the sample variogram vario_Ca_spec_a &lt;- sample.variogram(object = fitlayera$Ca_spec, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_Ca_spec_a$lag.dist, y = vario_Ca_spec_a$gamma, ylim = c(0, max(vario_Ca_spec_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;Ca, vis-NIR augmented - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_Ca_spec_a &lt;- c(variance = 112, nugget = 1, scale = 1023) ## Fit Ca_spec_a &lt;- georob(Ca ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_Ca_spec_a, fit.param = fitparam, tuning.psi = tpsi) summary(Ca_spec_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;Ca_spec&quot;, varparamames] &lt;- Ca_spec_a$variogram.object[[1]]$param[varparamames] \\(alr(clay)\\) (vis-NIR augmented) ## Check the sample variogram vario_alr_Clay_spec_a &lt;- sample.variogram(object = fitlayera$alr_Clay_spec, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Clay_spec_a$lag.dist, y = vario_alr_Clay_spec_a$gamma, ylim = c(0, max(vario_alr_Clay_spec_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(clay) vis-NIR augmented - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Clay_spec_a &lt;- c(variance = 1.134, nugget = 0.1, scale = 955) ## Fit alr_Clay_spec_a &lt;- georob(alr_Clay_spec ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Clay_spec_a, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Clay_spec_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;alr_Clay_spec&quot;, varparamames] &lt;- alr_Clay_spec_a$variogram.object[[1]]$param[varparamames] \\(alr(silt)\\) (vis-NIR augmented) ## Check the sample variogram vario_alr_Silt_spec_a &lt;- sample.variogram(object = fitlayera$alr_Silt_spec, locations = fitlayera[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Silt_spec_a$lag.dist, y = vario_alr_Silt_spec_a$gamma, ylim = c(0, max(vario_alr_Silt_spec_a$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(silt) vis-NIR augmented - Layer A&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Silt_spec_a &lt;- c(variance = 0.845, nugget = 0.1, scale = 1282) ## Fit alr_Silt_spec_a &lt;- georob(alr_Silt_spec ~ 1, data = fitlayera, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Silt_spec_a, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Silt_spec_a) ## Store the variogram parameters variogtablea[variogtablea$properties == &quot;alr_Silt_spec&quot;, varparamames] &lt;- alr_Silt_spec_a$variogram.object[[1]]$param[varparamames] 9.1.2.2 Layer B Exchangeable Ca2+ (vis-NIR augmented) … ## Check the sample variogram vario_Ca_spec_b &lt;- sample.variogram(object = fitlayerb$Ca_spec, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_Ca_spec_b$lag.dist, y = vario_Ca_spec_b$gamma, ylim = c(0, max(vario_Ca_spec_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;Ca, vis-NIR augmented - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_Ca_spec_b &lt;- c(variance = 142, nugget = 0.1, scale = 1025) ## Fit Ca_spec_b &lt;- georob(Ca ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_Ca_spec_b, fit.param = fitparam, tuning.psi = tpsi) summary(Ca_spec_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;Ca_spec&quot;, varparamames] &lt;- Ca_spec_b$variogram.object[[1]]$param[varparamames] \\(alr(clay)\\) (vis-NIR augmented) ## Check the sample variogram vario_alr_Clay_spec_b &lt;- sample.variogram(object = fitlayerb$alr_Clay_spec, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Clay_spec_b$lag.dist, y = vario_alr_Clay_spec_b$gamma, ylim = c(0, max(vario_alr_Clay_spec_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(clay) vis-NIR augmented - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Clay_spec_b &lt;- c(variance = 1.21, nugget = 0.1, scale = 1000, alpha = 0.68) ## Fit alr_Clay_spec_b &lt;- georob(alr_Clay_spec ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Clay_spec_b, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Clay_spec_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;alr_Clay_spec&quot;, varparamames] &lt;- alr_Clay_spec_b$variogram.object[[1]]$param[varparamames] \\(alr(silt)\\) (vis-NIR augmented) ## Check the sample variogram vario_alr_Silt_spec_b &lt;- sample.variogram(object = fitlayerb$alr_Silt_spec, locations = fitlayerb[, c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;)], lag.dist.def = lagdist, estimator = &quot;matheron&quot;) ## Plot the sample variogram plot(x = vario_alr_Silt_spec_b$lag.dist, y = vario_alr_Silt_spec_b$gamma, ylim = c(0, max(vario_alr_Silt_spec_b$gamma)), xlab = &quot;lag distance&quot;, ylab = &quot;gamma&quot;, pch = 16, col = &quot;red&quot;, main = &quot;alr(silt) vis-NIR augmented - Layer B&quot;) grid() ## Check the plot above to select some starting values of the variogram ## parameters startp_alr_Silt_spec_b &lt;- c(variance = 1.43, nugget = 0.1, scale = 850, alpha = 0.63) ## Fit alr_Silt_spec_b &lt;- georob(alr_Silt_spec ~ 1, data = fitlayerb, locations = ~POINT_X + POINT_Y, variogram.model = varmodel, param = startp_alr_Silt_spec_b, fit.param = fitparam, tuning.psi = tpsi) summary(alr_Silt_spec_b) ## Store the variogram parameters variogtableb[variogtableb$properties == &quot;alr_Silt_spec&quot;, varparamames] &lt;- alr_Silt_spec_b$variogram.object[[1]]$param[varparamames] 9.2 Accounting for vis-NIR model errors in the spatial models Our vis-NIR models (used to create the vis-NIR augmented data) have an error which we estimated in previous sections and which is stored in the residualvariances object. The uncertainty of these models was propagated through spatial predictions in order to obtain more realistic performance results of the spatial predictions. We followed the approach given in Viscarra Rossel et al. (2016b) where the variance of the residuals of the vis-NIR predictions at each layer is used for propagating the vis-NIR erros (see section Spectroscopicmodel error in our paper). For the vis-NIR augmented data we assume that the uncertainty of the whole set is given by the variances of the predictions. However, this dataset contains both, laboratory data (aprox. 26%) and vis-NIR predicted data (aprox. 74%), therefore the variance we assume here is expected to be larger than the actual one in the augmented data. You can try to account for this. ## Add the residual variances to the snugget parameter of the variograms ## corresponding to the vis-NIR augmented data Note that snugget was 0 for ## all the fitted variograms Ca (layer A) variogtablea$snugget[variogtablea$properties == &quot;Ca_spec&quot;] &lt;- residualvariances$layerA[residualvariances$Property == &quot;Ca&quot;] ## alr(clay) (layer A) variogtablea$snugget[variogtablea$properties == &quot;alr_Clay_spec&quot;] &lt;- residualvariances$layerA[residualvariances$Property == &quot;alr_Clay&quot;] ## alr(silt) (layer A) variogtablea$snugget[variogtablea$properties == &quot;alr_Silt_spec&quot;] &lt;- residualvariances$layerA[residualvariances$Property == &quot;alr_Silt&quot;] ## Ca (layer B) variogtableb$snugget[variogtableb$properties == &quot;Ca_spec&quot;] &lt;- residualvariances$layerB[residualvariances$Property == &quot;Ca&quot;] ## alr(clay) (layer B) variogtableb$snugget[variogtableb$properties == &quot;alr_Clay_spec&quot;] &lt;- residualvariances$layerB[residualvariances$Property == &quot;alr_Clay&quot;] ## alr(silt) (layer B) variogtableb$snugget[variogtableb$properties == &quot;alr_Silt_spec&quot;] &lt;- residualvariances$layerB[residualvariances$Property == &quot;alr_Silt&quot;] 9.3 Validation of the spatial models Create a data.frame for each layer to store the predictions in the validation set and another two to store the validation results… ## The final variables to be predicted. In the case of Clay, silt and Sand ## they are estimated from the predictions of alr(clay) and alr(silt) bprops &lt;- c(&quot;Ca&quot;, &quot;Clay&quot;, &quot;Silt&quot;, &quot;Sand&quot;, &quot;Ca_spec&quot;, &quot;Clay_spec&quot;, &quot;Silt_spec&quot;, &quot;Sand_spec&quot;) ## a quick function to create columns of a given length (lg) idfcol &lt;- function(x, lg) { data.frame(lg, fix.empty.names = FALSE) } ## object where the spatial predictions will be stored sppredsvala &lt;- sapply(c(&quot;ID&quot;, bprops), FUN = idfcol, lg = rep(NA, nrow(vallayera))) sppredsvala &lt;- data.frame(do.call(&quot;cbind&quot;, sppredsvala)) sppredsvala$ID &lt;- vallayera$ID sppredsvalb &lt;- sapply(c(&quot;ID&quot;, bprops), FUN = idfcol, lg = rep(NA, nrow(vallayerb))) sppredsvalb &lt;- data.frame(do.call(&quot;cbind&quot;, sppredsvalb)) sppredsvalb$ID &lt;- vallayerb$ID ## object where the validation of the spatial predictions will be stored spvala &lt;- data.frame(properties = bprops, data = rep(c(&quot;Laboratory&quot;, &quot;vis-NIR augmented&quot;), each = length(bprops)/2), R2 = NA, RMSE = NA, ME = NA) spvalb &lt;- spvala 9.3.1 Laboratory-based data 9.3.1.1 Layer A Exchangeable Ca2+… pred_Ca_lab_a &lt;- predict(Ca_lab_a, newdata = as.data.frame(vallayera), control = gcntrl) sppredsvala$Ca &lt;- pred_Ca_lab_a$pred$pred spvala$R2[spvala$properties == &quot;Ca&quot;] &lt;- cor(vallayera$Ca, sppredsvala$Ca)^2 spvala$RMSE[spvala$properties == &quot;Ca&quot;] &lt;- mean((vallayera$Ca - sppredsvala$Ca)^2)^0.5 spvala$ME[spvala$properties == &quot;Ca&quot;] &lt;- mean(vallayera$Ca - sppredsvala$Ca) Estimation of clay, silt and sand contents from the predictions of \\(alr(clay)\\) and \\(alr(silt)\\)… ## predict alr(clay) pred_alr_Clay_lab_a &lt;- predict(alr_Clay_lab_a, newdata = as.data.frame(vallayera), control = gcntrl) ## predict alr(silt) pred_alr_Silt_lab_a &lt;- predict(alr_Silt_lab_a, newdata = as.data.frame(vallayera), control = gcntrl) ## Back-transfrom to clay, silt and sand contents dvr.Silt &lt;- exp(pred_alr_Silt_lab_a$pred$pred + (0.5 * (pred_alr_Silt_lab_a$pred$var.target - pred_alr_Silt_lab_a$pred$cov.pred.target))) dvr.Clay &lt;- exp(pred_alr_Clay_lab_a$pred$pred + (0.5 * (pred_alr_Clay_lab_a$pred$var.target - pred_alr_Clay_lab_a$pred$cov.pred.target))) dvn.pred &lt;- 1 + dvr.Silt + dvr.Clay ## Store the back-transformed values sppredsvala$Clay &lt;- 100 * (dvr.Clay/dvn.pred) sppredsvala$Silt &lt;- 100 * (dvr.Silt/dvn.pred) sppredsvala$Sand &lt;- 100/dvn.pred ## Estimate the validation parameters for Clay spvala$R2[spvala$properties == &quot;Clay&quot;] &lt;- cor(vallayera$Clay, sppredsvala$Clay)^2 spvala$RMSE[spvala$properties == &quot;Clay&quot;] &lt;- mean((vallayera$Clay - sppredsvala$Clay)^2)^0.5 spvala$ME[spvala$properties == &quot;Clay&quot;] &lt;- mean(vallayera$Clay - sppredsvala$Clay) ## Silt spvala$R2[spvala$properties == &quot;Silt&quot;] &lt;- cor(vallayera$Silt, sppredsvala$Silt)^2 spvala$RMSE[spvala$properties == &quot;Silt&quot;] &lt;- mean((vallayera$Silt - sppredsvala$Silt)^2)^0.5 spvala$ME[spvala$properties == &quot;Silt&quot;] &lt;- mean(vallayera$Silt - sppredsvala$Silt) ## Sand spvala$R2[spvala$properties == &quot;Sand&quot;] &lt;- cor(vallayera$Sand, sppredsvala$Sand)^2 spvala$RMSE[spvala$properties == &quot;Sand&quot;] &lt;- mean((vallayera$Sand - sppredsvala$Sand)^2)^0.5 spvala$ME[spvala$properties == &quot;Sand&quot;] &lt;- mean(vallayera$Sand - sppredsvala$Sand) 9.3.1.2 Layer B Exchangeable Ca2+… pred_Ca_lab_b &lt;- predict(Ca_lab_b, newdata = as.data.frame(vallayerb), control = gcntrl) sppredsvalb$Ca &lt;- pred_Ca_lab_b$pred$pred spvalb$R2[spvala$properties == &quot;Ca&quot;] &lt;- cor(vallayerb$Ca, sppredsvalb$Ca)^2 spvalb$RMSE[spvala$properties == &quot;Ca&quot;] &lt;- mean((vallayerb$Ca - sppredsvalb$Ca)^2)^0.5 spvalb$ME[spvala$properties == &quot;Ca&quot;] &lt;- mean(vallayerb$Ca - sppredsvalb$Ca) Estimation of clay, silt and sand contents from the predictions of \\(alr(clay)\\) and \\(alr(silt)\\)… ## predict alr(clay) pred_alr_Clay_lab_b &lt;- predict(alr_Clay_lab_b, newdata = as.data.frame(vallayerb), control = gcntrl) ## predict alr(silt) pred_alr_Silt_lab_b &lt;- predict(alr_Silt_lab_b, newdata = as.data.frame(vallayerb), control = gcntrl) ## Back-transfrom to clay, silt and sand contents dvr.Silt &lt;- exp(pred_alr_Silt_lab_b$pred$pred + (0.5 * (pred_alr_Silt_lab_b$pred$var.target - pred_alr_Silt_lab_b$pred$cov.pred.target))) dvr.Clay &lt;- exp(pred_alr_Clay_lab_b$pred$pred + (0.5 * (pred_alr_Clay_lab_b$pred$var.target - pred_alr_Clay_lab_b$pred$cov.pred.target))) dvn.pred &lt;- 1 + dvr.Silt + dvr.Clay ## Store the back-transformed values sppredsvalb$Clay &lt;- 100 * (dvr.Clay/dvn.pred) sppredsvalb$Silt &lt;- 100 * (dvr.Silt/dvn.pred) sppredsvalb$Sand &lt;- 100/dvn.pred ## Estimate the validation parameters for Clay spvalb$R2[spvala$properties == &quot;Clay&quot;] &lt;- cor(vallayerb$Clay, sppredsvalb$Clay)^2 spvalb$RMSE[spvala$properties == &quot;Clay&quot;] &lt;- mean((vallayerb$Clay - sppredsvalb$Clay)^2)^0.5 spvalb$ME[spvala$properties == &quot;Clay&quot;] &lt;- mean(vallayerb$Clay - sppredsvalb$Clay) ## Silt spvalb$R2[spvala$properties == &quot;Silt&quot;] &lt;- cor(vallayerb$Silt, sppredsvalb$Silt)^2 spvalb$RMSE[spvala$properties == &quot;Silt&quot;] &lt;- mean((vallayerb$Silt - sppredsvalb$Silt)^2)^0.5 spvalb$ME[spvala$properties == &quot;Silt&quot;] &lt;- mean(vallayerb$Silt - sppredsvalb$Silt) ## Sand spvalb$R2[spvala$properties == &quot;Sand&quot;] &lt;- cor(vallayerb$Sand, sppredsvalb$Sand)^2 spvalb$RMSE[spvala$properties == &quot;Sand&quot;] &lt;- mean((vallayerb$Sand - sppredsvalb$Sand)^2)^0.5 spvalb$ME[spvala$properties == &quot;Sand&quot;] &lt;- mean(vallayerb$Sand - sppredsvalb$Sand) 9.3.2 Vis-NIR augmented-based data Here, for the spatial predictions in the vis-NIR augmented dataset we use the variogram parameters which include the residual variances of the vis-NIR models. For this we use the paramargument of the predict function in the georob pacakge. #### Layer A Exchangeable Ca2+… pred_Ca_spec_a &lt;- predict(Ca_spec_a, newdata = as.data.frame(vallayera), control = gcntrl, param = unlist(variogtablea[variogtablea$properties == &quot;Ca_spec&quot;, varparamames])) sppredsvala$Ca_spec &lt;- pred_Ca_spec_a$pred$pred spvala$R2[spvala$properties == &quot;Ca_spec&quot;] &lt;- cor(vallayera$Ca, sppredsvala$Ca_spec)^2 spvala$RMSE[spvala$properties == &quot;Ca_spec&quot;] &lt;- mean((vallayera$Ca - sppredsvala$Ca_spec)^2)^0.5 spvala$ME[spvala$properties == &quot;Ca_spec&quot;] &lt;- mean(vallayera$Ca - sppredsvala$Ca_spec) Estimation of clay, silt and sand contents from the predictions of \\(alr(clay)\\) and \\(alr(silt)\\)… ## predict alr(clay) pred_alr_Clay_spec_a &lt;- predict(alr_Clay_spec_a, newdata = as.data.frame(vallayera), control = gcntrl, param = unlist(variogtablea[variogtablea$properties == &quot;alr_Clay_spec&quot;, varparamames])) ## predict alr(silt) pred_alr_Silt_spec_a &lt;- predict(alr_Silt_spec_a, newdata = as.data.frame(vallayera), control = gcntrl, param = unlist(variogtablea[variogtablea$properties == &quot;alr_Silt_spec&quot;, varparamames])) ## Back-transfrom to clay, silt and sand contents dvr.Silt &lt;- exp(pred_alr_Silt_spec_a$pred$pred + (0.5 * (pred_alr_Silt_spec_a$pred$var.target - pred_alr_Silt_spec_a$pred$cov.pred.target))) dvr.Clay &lt;- exp(pred_alr_Clay_spec_a$pred$pred + (0.5 * (pred_alr_Clay_spec_a$pred$var.target - pred_alr_Clay_spec_a$pred$cov.pred.target))) dvn.pred &lt;- 1 + dvr.Silt + dvr.Clay ## Store the back-transformed values sppredsvala$Clay_spec &lt;- 100 * (dvr.Clay/dvn.pred) sppredsvala$Silt_spec &lt;- 100 * (dvr.Silt/dvn.pred) sppredsvala$Sand_spec &lt;- 100/dvn.pred ## Estimate the validation parameters for Clay spvala$R2[spvala$properties == &quot;Clay_spec&quot;] &lt;- cor(vallayera$Clay, sppredsvala$Clay_spec)^2 spvala$RMSE[spvala$properties == &quot;Clay_spec&quot;] &lt;- mean((vallayera$Clay - sppredsvala$Clay_spec)^2)^0.5 spvala$ME[spvala$properties == &quot;Clay_spec&quot;] &lt;- mean(vallayera$Clay - sppredsvala$Clay_spec) ## Silt spvala$R2[spvala$properties == &quot;Silt_spec&quot;] &lt;- cor(vallayera$Silt, sppredsvala$Silt_spec)^2 spvala$RMSE[spvala$properties == &quot;Silt_spec&quot;] &lt;- mean((vallayera$Silt - sppredsvala$Silt_spec)^2)^0.5 spvala$ME[spvala$properties == &quot;Silt_spec&quot;] &lt;- mean(vallayera$Silt - sppredsvala$Silt_spec) ## Sand spvala$R2[spvala$properties == &quot;Sand_spec&quot;] &lt;- cor(vallayera$Sand, sppredsvala$Sand_spec)^2 spvala$RMSE[spvala$properties == &quot;Sand_spec&quot;] &lt;- mean((vallayera$Sand - sppredsvala$Sand_spec)^2)^0.5 spvala$ME[spvala$properties == &quot;Sand_spec&quot;] &lt;- mean(vallayera$Sand - sppredsvala$Sand_spec) 9.3.2.1 Layer B Exchangeable Ca2+… pred_Ca_spec_b &lt;- predict(Ca_spec_b, newdata = as.data.frame(vallayerb), control = gcntrl, param = unlist(variogtableb[variogtableb$properties == &quot;Ca_spec&quot;, varparamames])) sppredsvalb$Ca_spec &lt;- pred_Ca_spec_b$pred$pred spvalb$R2[spvala$properties == &quot;Ca_spec&quot;] &lt;- cor(vallayerb$Ca, sppredsvalb$Ca_spec)^2 spvalb$RMSE[spvala$properties == &quot;Ca_spec&quot;] &lt;- mean((vallayerb$Ca - sppredsvalb$Ca_spec)^2)^0.5 spvalb$ME[spvala$properties == &quot;Ca_spec&quot;] &lt;- mean(vallayerb$Ca - sppredsvalb$Ca_spec) Estimation of clay, silt and sand contents from the predictions of \\(alr(clay)\\) and \\(alr(silt)\\)… ## predict alr(clay) pred_alr_Clay_spec_b &lt;- predict(alr_Clay_spec_b, newdata = as.data.frame(vallayerb), control = gcntrl, param = unlist(variogtableb[variogtableb$properties == &quot;alr_Clay_spec&quot;, varparamames])) ## predict alr(silt) pred_alr_Silt_spec_b &lt;- predict(alr_Silt_spec_b, newdata = as.data.frame(vallayerb), control = gcntrl, param = unlist(variogtableb[variogtableb$properties == &quot;alr_Silt_spec&quot;, varparamames])) ## Back-transfrom to clay, silt and sand contents dvr.Silt &lt;- exp(pred_alr_Silt_spec_b$pred$pred + (0.5 * (pred_alr_Silt_spec_b$pred$var.target - pred_alr_Silt_spec_b$pred$cov.pred.target))) dvr.Clay &lt;- exp(pred_alr_Clay_spec_b$pred$pred + (0.5 * (pred_alr_Clay_spec_b$pred$var.target - pred_alr_Clay_spec_b$pred$cov.pred.target))) dvn.pred &lt;- 1 + dvr.Silt + dvr.Clay ## Store the back-transformed values sppredsvalb$Clay_spec &lt;- 100 * (dvr.Clay/dvn.pred) sppredsvalb$Silt_spec &lt;- 100 * (dvr.Silt/dvn.pred) sppredsvalb$Sand_spec &lt;- 100/dvn.pred ## Estimate the validation parameters for Clay spvalb$R2[spvala$properties == &quot;Clay_spec&quot;] &lt;- cor(vallayerb$Clay, sppredsvalb$Clay_spec)^2 spvalb$RMSE[spvala$properties == &quot;Clay_spec&quot;] &lt;- mean((vallayerb$Clay - sppredsvalb$Clay_spec)^2)^0.5 spvalb$ME[spvala$properties == &quot;Clay_spec&quot;] &lt;- mean(vallayerb$Clay - sppredsvalb$Clay_spec) ## Silt spvalb$R2[spvala$properties == &quot;Silt_spec&quot;] &lt;- cor(vallayerb$Silt, sppredsvalb$Silt_spec)^2 spvalb$RMSE[spvala$properties == &quot;Silt_spec&quot;] &lt;- mean((vallayerb$Silt - sppredsvalb$Silt_spec)^2)^0.5 spvalb$ME[spvala$properties == &quot;Silt_spec&quot;] &lt;- mean(vallayerb$Silt - sppredsvalb$Silt_spec) ## Sand spvalb$R2[spvala$properties == &quot;Sand_spec&quot;] &lt;- cor(vallayerb$Sand, sppredsvalb$Sand_spec)^2 spvalb$RMSE[spvala$properties == &quot;Sand_spec&quot;] &lt;- mean((vallayerb$Sand - sppredsvalb$Sand_spec)^2)^0.5 spvalb$ME[spvala$properties == &quot;Sand_spec&quot;] &lt;- mean(vallayerb$Sand - sppredsvalb$Sand_spec) 9.4 Mapping Now that we have validated the spatial models for both laboratory data and vis-NIR augmented data, we proceed to produce the maps of each property at each layer. First we have to read the polygon corresponding to the study area. Download the polygon to your working directory. This is a R object of class (SpatialPolygonsDataFrame of the package sp) which contains the polygon of the study area. Click here to download it. If you saved the file in your working directory you can: polyfile &lt;- file(&quot;polygon.rds&quot;) shape &lt;- readRDS(polyfile) shape or… polyfile &lt;- file(&quot;https://github.com/l-ramirez-lopez/VNIR_spectroscopy_for_robust_soil_mapping/raw/master/polygon.rds&quot;) shape &lt;- readRDS(polyfile) shape Now create a template for the spatial predictions ## function to convert polygon to raster pol2raster &lt;- function(r, nrows = 10, ncols = 10) { ext &lt;- raster(extent(r), nrows, ncols) crs(ext) &lt;- crs(r) fr &lt;- rasterize(r, ext, field = 1, update = TRUE) return(fr) } rncol &lt;- (extent(shape)@ymax - extent(shape)@ymin)/10 rnrow &lt;- (extent(shape)@xmax - extent(shape)@xmin)/10 rasg &lt;- pol2raster(shape, rncol, rnrow) ## resolution here you can choose the resolution for the predicted maps for ## our paper we set this value to 10, however here we will set it to 50 for ## the sake of memory if you have a decent machine you can go finer mresolution &lt;- 50 rasg &lt;- raster::resample(rasg, raster(resolution = mresolution, ext = extent(shape)), method = &quot;ngb&quot;) rasg plot(rasg) ## and here we have our template sppx &lt;- as(rasg, &quot;SpatialPixelsDataFrame&quot;) colnames(sppx@coords) &lt;- c(&quot;POINT_X&quot;, &quot;POINT_Y&quot;) Create a data.frame for each layer to store the predicted values for the maps spatialpredsa &lt;- data.frame(matrix(NA, nrow(sppx), length(bprops))) colnames(spatialpredsa) &lt;- bprops spatialpredsb &lt;- spatialpredsa 9.4.1 Laboratory-based data 9.4.1.1 Layer A Exchangeable Ca2+… spatialpredsa$Ca &lt;- predict(Ca_lab_a, newdata = sppx)$pred Clay, sand and silt contents.. alr_Clay_lab_a_map &lt;- predict(alr_Clay_lab_a, newdata = sppx) alr_Silt_lab_a_map &lt;- predict(alr_Silt_lab_a, newdata = sppx) spatialpredsa$Clay &lt;- 100 * exp(alr_Clay_lab_a_map$pred)/(1 + exp(alr_Silt_lab_a_map$pred) + exp(alr_Clay_lab_a_map$pred)) spatialpredsa$Silt &lt;- 100 * exp(alr_Silt_lab_a_map$pred)/(1 + exp(alr_Silt_lab_a_map$pred) + exp(alr_Clay_lab_a_map$pred)) spatialpredsa$Sand &lt;- 100/(1 + exp(alr_Silt_lab_a_map$pred) + exp(alr_Clay_lab_a_map$pred)) 9.4.1.2 Layer B Exchangeable Ca2+… spatialpredsb$Ca &lt;- predict(Ca_lab_b, newdata = sppx)$pred Clay, sand and silt contents.. alr_Clay_lab_b_map &lt;- predict(alr_Clay_lab_b, newdata = sppx) alr_Silt_lab_b_map &lt;- predict(alr_Silt_lab_b, sppx) spatialpredsb$Clay &lt;- 100 * exp(alr_Clay_lab_b_map$pred)/(1 + exp(alr_Silt_lab_b_map$pred) + exp(alr_Clay_lab_b_map$pred)) spatialpredsb$Silt &lt;- 100 * exp(alr_Silt_lab_b_map$pred)/(1 + exp(alr_Silt_lab_b_map$pred) + exp(alr_Clay_lab_b_map$pred)) spatialpredsb$Sand &lt;- 100/(1 + exp(alr_Silt_lab_b_map$pred) + exp(alr_Clay_lab_b_map$pred)) 9.4.2 Vis-NIR augmented-based data 9.4.2.1 Layer A Exchangeable Ca2+… spatialpredsa$Ca_spec &lt;- predict(Ca_spec_a, newdata = sppx, param = unlist(variogtablea[variogtablea$properties == &quot;Ca_spec&quot;, varparamames]))$pred Clay, sand and silt contents.. alr_Clay_spec_a_map &lt;- predict(alr_Clay_spec_a, newdata = sppx, param = unlist(variogtablea[variogtablea$properties == &quot;alr_Silt_spec&quot;, varparamames])) alr_Silt_spec_a_map &lt;- predict(alr_Silt_spec_a, newdata = sppx, param = unlist(variogtablea[variogtablea$properties == &quot;alr_Clay_spec&quot;, varparamames])) spatialpredsa$Clay_spec &lt;- 100 * exp(alr_Clay_spec_a_map$pred)/(1 + exp(alr_Silt_spec_a_map$pred) + exp(alr_Clay_spec_a_map$pred)) spatialpredsa$Silt_spec &lt;- 100 * exp(alr_Silt_spec_a_map$pred)/(1 + exp(alr_Silt_spec_a_map$pred) + exp(alr_Clay_spec_a_map$pred)) spatialpredsa$Sand_spec &lt;- 100/(1 + exp(alr_Silt_spec_a_map$pred) + exp(alr_Clay_spec_a_map$pred)) 9.4.2.2 Layer B Exchangeable Ca2+… spatialpredsb$Ca_spec &lt;- predict(Ca_spec_b, newdata = sppx, param = unlist(variogtableb[variogtableb$properties == &quot;Ca_spec&quot;, varparamames]))$pred Clay, sand and silt contents.. alr_Clay_spec_b_map &lt;- predict(alr_Clay_spec_b, newdata = sppx, param = unlist(variogtableb[variogtableb$properties == &quot;alr_Silt_spec&quot;, varparamames])) alr_Silt_spec_b_map &lt;- predict(alr_Silt_spec_b, newdata = sppx, param = unlist(variogtableb[variogtableb$properties == &quot;alr_Clay_spec&quot;, varparamames])) spatialpredsb$Clay_spec &lt;- 100 * exp(alr_Clay_spec_b_map$pred)/(1 + exp(alr_Silt_spec_b_map$pred) + exp(alr_Clay_spec_b_map$pred)) spatialpredsb$Silt_spec &lt;- 100 * exp(alr_Silt_spec_b_map$pred)/(1 + exp(alr_Silt_spec_b_map$pred) + exp(alr_Clay_spec_b_map$pred)) spatialpredsb$Sand_spec &lt;- 100/(1 + exp(alr_Silt_spec_b_map$pred) + exp(alr_Clay_spec_b_map$pred)) 9.4.3 Plots ## compute the differences between maps (layer) spatialpredsa$Cadiff &lt;- spatialpredsa$Ca - spatialpredsa$Ca_spec spatialpredsa$Claydiff &lt;- spatialpredsa$Clay - spatialpredsa$Clay_spec spatialpredsa$Siltdiff &lt;- spatialpredsa$Silt - spatialpredsa$Silt_spec spatialpredsa$Sanddiff &lt;- spatialpredsa$Sand - spatialpredsa$Sand_spec ## compute the differences between maps (layer B) spatialpredsb$Cadiff &lt;- spatialpredsb$Ca - spatialpredsb$Ca_spec spatialpredsb$Claydiff &lt;- spatialpredsb$Clay - spatialpredsb$Clay_spec spatialpredsb$Siltdiff &lt;- spatialpredsb$Silt - spatialpredsb$Silt_spec spatialpredsb$Sanddiff &lt;- spatialpredsb$Sand - spatialpredsb$Sand_spec Create a new data.frame for plotting purposes pred_layer_a &lt;- data.frame(rbind(sppx@coords, sppx@coords, sppx@coords), layer = &quot;Depth A (0 - 0.2 m)&quot;, method = c(rep(&quot;Laboratory-based&quot;, nrow(spatialpredsa)), rep(&quot;vis-NIR augmented&quot;, nrow(spatialpredsa)), rep(&quot;Differences between maps&quot;, nrow(spatialpredsa))), Ca = unlist(spatialpredsa[, c(&quot;Ca&quot;, &quot;Ca_spec&quot;, &quot;Cadiff&quot;)]), Clay = unlist(spatialpredsa[, c(&quot;Clay&quot;, &quot;Clay_spec&quot;, &quot;Claydiff&quot;)]), Silt = unlist(spatialpredsa[, c(&quot;Silt&quot;, &quot;Silt_spec&quot;, &quot;Siltdiff&quot;)]), Sand = unlist(spatialpredsa[, c(&quot;Sand&quot;, &quot;Sand_spec&quot;, &quot;Sanddiff&quot;)])) pred_layer_b &lt;- data.frame(rbind(sppx@coords, sppx@coords, sppx@coords), layer = &quot;Depth B (0.8 - 1.0 m)&quot;, method = c(rep(&quot;Laboratory-based&quot;, nrow(spatialpredsb)), rep(&quot;vis-NIR augmented&quot;, nrow(spatialpredsb)), rep(&quot;Differences between maps&quot;, nrow(spatialpredsb))), Ca = unlist(spatialpredsb[, c(&quot;Ca&quot;, &quot;Ca_spec&quot;, &quot;Cadiff&quot;)]), Clay = unlist(spatialpredsb[, c(&quot;Clay&quot;, &quot;Clay_spec&quot;, &quot;Claydiff&quot;)]), Silt = unlist(spatialpredsb[, c(&quot;Silt&quot;, &quot;Silt_spec&quot;, &quot;Siltdiff&quot;)]), Sand = unlist(spatialpredsb[, c(&quot;Sand&quot;, &quot;Sand_spec&quot;, &quot;Sanddiff&quot;)])) finalmapping &lt;- data.frame(rbind(pred_layer_a, pred_layer_b)) Define palette of color for the plot myPalette &lt;- colorRampPalette(rev(brewer.pal(11, &quot;Spectral&quot;)), space = &quot;Lab&quot;) Define some ggplot parameters common to all plots gfg &lt;- facet_grid(layer ~ method) gscf &lt;- scale_fill_gradientn(colours = myPalette(30)) gcoord &lt;- coord_equal() gtheme &lt;- theme_bw() + theme(legend.position = &quot;top&quot;) + theme(axis.title.y = element_text(colour = grey(0.2), size = 25), axis.text.y = element_text(angle = 0, vjust = 0.5, hjust = 0.5, size = 16), legend.title = element_text(colour = &quot;black&quot;, size = 25)) + theme(axis.title.x = element_text(colour = grey(0.2), size = 25), axis.text.x = element_text(angle = 0, vjust = 0, size = 16)) + theme(legend.text = element_text(colour = grey(0.2), size = 15), legend.key.size = unit(0.95, &quot;cm&quot;)) + theme(strip.background = element_rect(fill = &quot;grey&quot;), strip.text.x = element_text(size = 25, colour = &quot;black&quot;, angle = 0), strip.text.y = element_text(size = 25, colour = &quot;black&quot;, angle = -90)) glabs &lt;- labs(y = &quot;Northings /m&quot;, x = &quot;Eastings /m&quot;) Create the maps for Ca2+ ca_map &lt;- ggplot(finalmapping[finalmapping$method != &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Ca)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = expression(paste(&quot;&quot;, Ca^&quot;++&quot;, &quot; &quot;/mmol[c], kg^-1, &quot; &quot;)))) ca_map_diff &lt;- ggplot(finalmapping[finalmapping$method == &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Ca)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = expression(paste(&quot;Difference&quot;, &quot; &quot;/mmol[c], kg^-1, &quot; &quot;)))) ca_map ca_map_diff Create the maps for \\(Clay\\) Clay_map &lt;- ggplot(finalmapping[finalmapping$method != &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Clay)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Clay content, % &quot;)) Clay_map_diff &lt;- ggplot(finalmapping[finalmapping$method == &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Clay)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Difference, %&quot;)) Clay_map Clay_map_diff Create the maps for \\(Silt\\) Silt_map &lt;- ggplot(finalmapping[finalmapping$method != &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Silt)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Silt content, % &quot;)) Silt_map_diff &lt;- ggplot(finalmapping[finalmapping$method == &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Silt)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Difference, %&quot;)) Silt_map Silt_map_diff Create the maps for \\(Sand\\) Sand_map &lt;- ggplot(finalmapping[finalmapping$method != &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Sand)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Sand content, % &quot;)) Sand_map_diff &lt;- ggplot(finalmapping[finalmapping$method == &quot;Differences between maps&quot;, ], aes(POINT_X, POINT_Y)) + geom_tile(aes(fill = Sand)) + gfg + gscf + gcoord + gtheme + glabs + guides(fill = guide_colourbar(title = &quot;Difference, %&quot;)) Sand_map Sand_map_diff "],
["references.html", "References", " References Odeh, I.O., Todd, A.J. &amp; Triantafilis, J. 2003. Spatial prediction of soil particle-size fractions as compositional data. Soil Science, 168, 501–515. Ramirez-Lopez, L., Wadoux, A.C., Franceschini, M.H.D., Terra, F.S., Marques, K.P.P., Sayão, V.M. and Demattê, J.A.M., 2019. Robust soil mapping at the farm scale with vis–NIR spectroscopy. European Journal of Soil Science. Viscarra Rossel, R., Brus, D.J., Lobsey, C., Shi, Z. &amp;McLachlan, G. 2016. Baseline estimates of soil organic carbon by proximal sensing: comparing design-based, model-assisted and model-based inference. Geoderma,265, 152–163. "]
]
